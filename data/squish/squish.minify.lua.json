{
  "_filename": "squish.minify.lua",
  "_isShebang": false,
  "_isSpec": false,
  "_isTest": false,
  "_path": "modules/squish/minify/squish.minify.lua",
  "edges": [ {
    "from": 0,
    "label": "implements",
    "to": 4
  }, {
    "from": 10,
    "label": "has",
    "to": 11
  }, {
    "from": 10,
    "label": "has",
    "to": 12
  }, {
    "from": 10,
    "label": "has",
    "to": 13
  }, {
    "from": 14,
    "label": "has",
    "to": 15
  }, {
    "from": 14,
    "label": "has",
    "to": 16
  }, {
    "from": 16,
    "label": "has",
    "to": 17
  }, {
    "from": 14,
    "label": "has",
    "to": 18
  }, {
    "from": 14,
    "label": "has",
    "to": 19
  }, {
    "from": 19,
    "label": "has",
    "to": 20
  }, {
    "from": 14,
    "label": "has",
    "to": 21
  }, {
    "from": 14,
    "label": "has",
    "to": 22
  }, {
    "from": 14,
    "label": "has",
    "to": 23
  }, {
    "from": 24,
    "label": "has",
    "to": 25
  }, {
    "from": 24,
    "label": "has",
    "to": 26
  }, {
    "from": 26,
    "label": "has",
    "to": 27
  }, {
    "from": 24,
    "label": "has",
    "to": 28
  }, {
    "from": 24,
    "label": "has",
    "to": 29
  }, {
    "from": 29,
    "label": "has",
    "to": 30
  }, {
    "from": 24,
    "label": "has",
    "to": 31
  }, {
    "from": 24,
    "label": "has",
    "to": 32
  }, {
    "from": 33,
    "label": "has",
    "to": 34
  }, {
    "from": 33,
    "label": "has",
    "to": 35
  }, {
    "from": 33,
    "label": "has",
    "to": 36
  }, {
    "from": 33,
    "label": "has",
    "to": 37
  }, {
    "from": 37,
    "label": "has",
    "to": 38
  }, {
    "from": 37,
    "label": "has",
    "to": 39
  }, {
    "from": 37,
    "label": "has",
    "to": 40
  }, {
    "from": 37,
    "label": "has",
    "to": 41
  }, {
    "from": 33,
    "label": "has",
    "to": 42
  }, {
    "from": 33,
    "label": "has",
    "to": 43
  }, {
    "from": 33,
    "label": "has",
    "to": 44
  }, {
    "from": 33,
    "label": "has",
    "to": 45
  }, {
    "from": 45,
    "label": "has",
    "to": 46
  }, {
    "from": 33,
    "label": "has",
    "to": 22
  }, {
    "from": 33,
    "label": "has",
    "to": 47
  }, {
    "from": 48,
    "label": "has",
    "to": 49
  }, {
    "from": 48,
    "label": "has",
    "to": 50
  }, {
    "from": 48,
    "label": "has",
    "to": 51
  }, {
    "from": 48,
    "label": "has",
    "to": 52
  }, {
    "from": 33,
    "label": "calls",
    "to": 55
  }, {
    "from": 10,
    "label": "calls",
    "to": 57
  }, {
    "from": 33,
    "label": "calls",
    "to": 53
  }, {
    "from": 33,
    "label": "calls",
    "to": 53
  }, {
    "from": 14,
    "label": "calls",
    "to": 59
  }, {
    "from": 24,
    "label": "calls",
    "to": 59
  }, {
    "from": 14,
    "label": "calls",
    "to": 10
  }, {
    "from": 14,
    "label": "calls",
    "to": 10
  }, {
    "from": 24,
    "label": "calls",
    "to": 10
  }, {
    "from": 24,
    "label": "calls",
    "to": 10
  }, {
    "from": 33,
    "label": "calls",
    "to": 56
  }, {
    "from": 33,
    "label": "calls",
    "to": 56
  }, {
    "from": 33,
    "label": "calls",
    "to": 8
  }, {
    "from": 48,
    "label": "calls",
    "to": 33
  }, {
    "from": 48,
    "label": "calls",
    "to": 14
  }, {
    "from": 33,
    "label": "calls",
    "to": 54
  }, {
    "from": 10,
    "label": "calls",
    "to": 58
  }, {
    "from": 7,
    "label": "calls",
    "to": 48
  }, {
    "from": 48,
    "label": "calls",
    "to": 24
  }, {
    "from": 33,
    "label": "calls",
    "to": 9
  }, {
    "from": 33,
    "label": "calls",
    "to": 9
  }, {
    "from": 7,
    "label": "declares",
    "to": 10
  }, {
    "from": 7,
    "label": "declares",
    "to": 14
  }, {
    "from": 7,
    "label": "declares",
    "to": 24
  }, {
    "from": 7,
    "label": "declares",
    "to": 33
  }, {
    "from": 7,
    "label": "declares",
    "to": 48
  }, {
    "from": 6,
    "label": "contains",
    "to": 60
  }, {
    "from": 6,
    "label": "contains",
    "to": 61
  }, {
    "from": 60,
    "label": "initializes",
    "to": 62
  }, {
    "from": 62,
    "label": "assigns",
    "to": 63
  }, {
    "from": 63,
    "label": "assigns",
    "to": 64
  }, {
    "from": 64,
    "label": "assigns",
    "to": 65
  }, {
    "from": 63,
    "label": "assigns",
    "to": 66
  }, {
    "from": 66,
    "label": "assigns",
    "to": 67
  }, {
    "from": 63,
    "label": "assigns",
    "to": 68
  }, {
    "from": 68,
    "label": "assigns",
    "to": 69
  }, {
    "from": 63,
    "label": "assigns",
    "to": 70
  }, {
    "from": 70,
    "label": "assigns",
    "to": 71
  }, {
    "from": 63,
    "label": "assigns",
    "to": 72
  }, {
    "from": 72,
    "label": "assigns",
    "to": 73
  }, {
    "from": 60,
    "label": "initializes",
    "to": 74
  }, {
    "from": 74,
    "label": "assigns",
    "to": 75
  }, {
    "from": 75,
    "label": "assigns",
    "to": 76
  }, {
    "from": 76,
    "label": "assigns",
    "to": 77
  }, {
    "from": 75,
    "label": "assigns",
    "to": 78
  }, {
    "from": 78,
    "label": "assigns",
    "to": 79
  }, {
    "from": 75,
    "label": "assigns",
    "to": 80
  }, {
    "from": 80,
    "label": "assigns",
    "to": 81
  }, {
    "from": 75,
    "label": "assigns",
    "to": 82
  }, {
    "from": 82,
    "label": "assigns",
    "to": 83
  }, {
    "from": 75,
    "label": "assigns",
    "to": 84
  }, {
    "from": 84,
    "label": "assigns",
    "to": 85
  }, {
    "from": 75,
    "label": "assigns",
    "to": 86
  }, {
    "from": 86,
    "label": "assigns",
    "to": 87
  }, {
    "from": 75,
    "label": "assigns",
    "to": 88
  }, {
    "from": 88,
    "label": "assigns",
    "to": 89
  }, {
    "from": 75,
    "label": "assigns",
    "to": 90
  }, {
    "from": 90,
    "label": "assigns",
    "to": 91
  }, {
    "from": 61,
    "label": "initializes",
    "to": 92
  }, {
    "from": 92,
    "label": "requires",
    "to": 2
  }, {
    "from": 61,
    "label": "initializes",
    "to": 93
  }, {
    "from": 93,
    "label": "requires",
    "to": 3
  }, {
    "from": 61,
    "label": "initializes",
    "to": 94
  }, {
    "from": 94,
    "label": "requires",
    "to": 5
  }, {
    "from": 61,
    "label": "initializes",
    "to": 95
  }, {
    "from": 95,
    "label": "requires",
    "to": 1
  } ],
  "nodes": [ {
    "id": 0,
    "text": "",
    "type": "file"
  }, {
    "id": 1,
    "text": "lparser",
    "type": "module"
  }, {
    "id": 2,
    "text": "optlex",
    "type": "module"
  }, {
    "id": 3,
    "text": "optparser",
    "type": "module"
  }, {
    "id": 4,
    "text": "squish",
    "type": "module"
  }, {
    "id": 5,
    "text": "llex",
    "type": "module"
  }, {
    "id": 6,
    "text": "squish",
    "type": "module"
  }, {
    "id": 7,
    "text": "",
    "type": "function container"
  }, {
    "id": 8,
    "text": "function parser()\r\n  open_func()\r\n  fs.is_vararg = true  -- main func. is always vararg\r\n  nextt()  -- read first token\r\n  chunk()\r\n  check(\"<eof>\")\r\n  close_func()\r\n  return globalinfo, localinfo\r\nend",
    "type": "function"
  }, {
    "id": 9,
    "text": "function optimize(option, toklist, semlist, toklnlist)\r\n  --------------------------------------------------------------------\r\n  -- set option flags\r\n  --------------------------------------------------------------------\r\n  local opt_comments = option[\"opt-comments\"]\r\n  local opt_whitespace = option[\"opt-whitespace\"]\r\n  local opt_emptylines = option[\"opt-emptylines\"]\r\n  local opt_eols = option[\"opt-eols\"]\r\n  local opt_strings = option[\"opt-strings\"]\r\n  local opt_numbers = option[\"opt-numbers\"]\r\n  local opt_keep = option.KEEP\r\n  opt_details = option.DETAILS and 0  -- upvalues for details display\r\n  print = print or base.print\r\n  if opt_eols then  -- forced settings, otherwise won't work properly\r\n    opt_comments = true\r\n    opt_whitespace = true\r\n    opt_emptylines = true\r\n  end\r\n  --------------------------------------------------------------------\r\n  -- variable initialization\r\n  --------------------------------------------------------------------\r\n  stoks, sinfos, stoklns                -- set source lists\r\n    = toklist, semlist, toklnlist\r\n  local i = 1                           -- token position\r\n  local tok, info                       -- current token\r\n  local prev    -- position of last grammar token\r\n                -- on same line (for TK_SPACE stuff)\r\n  --------------------------------------------------------------------\r\n  -- changes a token, info pair\r\n  --------------------------------------------------------------------\r\n  local function settoken(tok, info, I)\r\n    I = I or i\r\n    stoks[I] = tok or \"\"\r\n    sinfos[I] = info or \"\"\r\n  end\r\n  --------------------------------------------------------------------\r\n  -- processing loop (PASS 1)\r\n  --------------------------------------------------------------------\r\n  while true do\r\n    tok, info = stoks[i], sinfos[i]\r\n    ----------------------------------------------------------------\r\n    local atstart = atlinestart(i)      -- set line begin flag\r\n    if atstart then prev = nil end\r\n    ----------------------------------------------------------------\r\n    if tok == \"TK_EOS\" then             -- end of stream/pass\r\n      break\r\n    ----------------------------------------------------------------\r\n    elseif tok == \"TK_KEYWORD\" or       -- keywords, identifiers,\r\n           tok == \"TK_NAME\" or          -- operators\r\n           tok == \"TK_OP\" then\r\n      -- TK_KEYWORD and TK_OP can't be optimized without a big\r\n      -- optimization framework; it would be more of an optimizing\r\n      -- compiler, not a source code compressor\r\n      -- TK_NAME that are locals needs parser to analyze/optimize\r\n      prev = i\r\n    ----------------------------------------------------------------\r\n    elseif tok == \"TK_NUMBER\" then      -- numbers\r\n      if opt_numbers then\r\n        do_number(i)  -- optimize\r\n      end\r\n      prev = i\r\n    ----------------------------------------------------------------\r\n    elseif tok == \"TK_STRING\" or        -- strings, long strings\r\n           tok == \"TK_LSTRING\" then\r\n      if opt_strings then\r\n        if tok == \"TK_STRING\" then\r\n          do_string(i)  -- optimize\r\n        else\r\n          do_lstring(i)  -- optimize\r\n        end\r\n      end\r\n      prev = i\r\n    ----------------------------------------------------------------\r\n    elseif tok == \"TK_COMMENT\" then     -- short comments\r\n      if opt_comments then\r\n        if i == 1 and sub(info, 1, 1) == \"#\" then\r\n          -- keep shbang comment, trim whitespace\r\n          do_comment(i)\r\n        else\r\n          -- safe to delete, as a TK_EOL (or TK_EOS) always follows\r\n          settoken()  -- remove entirely\r\n        end\r\n      elseif opt_whitespace then        -- trim whitespace only\r\n        do_comment(i)\r\n      end\r\n    ----------------------------------------------------------------\r\n    elseif tok == \"TK_LCOMMENT\" then    -- long comments\r\n      if keep_lcomment(opt_keep, info) then\r\n        ------------------------------------------------------------\r\n        -- if --keep, we keep a long comment if <msg> is found;\r\n        -- this is a feature to keep copyright or license texts\r\n        if opt_whitespace then          -- trim whitespace only\r\n          do_lcomment(i)\r\n        end\r\n        prev = i\r\n      elseif opt_comments then\r\n        local eols = commenteols(info)\r\n        ------------------------------------------------------------\r\n        -- prepare opt_emptylines case first, if a disposable token\r\n        -- follows, current one is safe to dump, else keep a space;\r\n        -- it is implied that the operation is safe for '-', because\r\n        -- current is a TK_LCOMMENT, and must be separate from a '-'\r\n        if is_faketoken[stoks[i + 1]] then\r\n          settoken()  -- remove entirely\r\n          tok = \"\"\r\n        else\r\n          settoken(\"TK_SPACE\", \" \")\r\n        end\r\n        ------------------------------------------------------------\r\n        -- if there are embedded EOLs to keep and opt_emptylines is\r\n        -- disabled, then switch the token into one or more EOLs\r\n        if not opt_emptylines and eols > 0 then\r\n          settoken(\"TK_EOL\", rep(\"\\n\", eols))\r\n        end\r\n        ------------------------------------------------------------\r\n        -- if optimizing whitespaces, force reinterpretation of the\r\n        -- token to give a chance for the space to be optimized away\r\n        if opt_whitespace and tok ~= \"\" then\r\n          i = i - 1  -- to reinterpret\r\n        end\r\n        ------------------------------------------------------------\r\n      else                              -- disabled case\r\n        if opt_whitespace then          -- trim whitespace only\r\n          do_lcomment(i)\r\n        end\r\n        prev = i\r\n      end\r\n    ----------------------------------------------------------------\r\n    elseif tok == \"TK_EOL\" then         -- line endings\r\n      if atstart and opt_emptylines then\r\n        settoken()  -- remove entirely\r\n      elseif info == \"\\r\\n\" or info == \"\\n\\r\" then\r\n        -- normalize the rest of the EOLs for CRLF/LFCR only\r\n        -- (note that TK_LCOMMENT can change into several EOLs)\r\n        settoken(\"TK_EOL\", \"\\n\")\r\n      end\r\n    ----------------------------------------------------------------\r\n    elseif tok == \"TK_SPACE\" then       -- whitespace\r\n      if opt_whitespace then\r\n        if atstart or atlineend(i) then\r\n          -- delete leading and trailing whitespace\r\n          settoken()  -- remove entirely\r\n        else\r\n          ------------------------------------------------------------\r\n          -- at this point, since leading whitespace have been removed,\r\n          -- there should be a either a real token or a TK_LCOMMENT\r\n          -- prior to hitting this whitespace; the TK_LCOMMENT case\r\n          -- only happens if opt_comments is disabled; so prev ~= nil\r\n          local ptok = stoks[prev]\r\n          if ptok == \"TK_LCOMMENT\" then\r\n            -- previous TK_LCOMMENT can abut with anything\r\n            settoken()  -- remove entirely\r\n          else\r\n            -- prev must be a grammar token; consecutive TK_SPACE\r\n            -- tokens is impossible when optimizing whitespace\r\n            local ntok = stoks[i + 1]\r\n            if is_faketoken[ntok] then\r\n              -- handle special case where a '-' cannot abut with\r\n              -- either a short comment or a long comment\r\n              if (ntok == \"TK_COMMENT\" or ntok == \"TK_LCOMMENT\") and\r\n                 ptok == \"TK_OP\" and sinfos[prev] == \"-\" then\r\n                -- keep token\r\n              else\r\n                settoken()  -- remove entirely\r\n              end\r\n            else--is_realtoken\r\n              -- check a pair of grammar tokens, if can abut, then\r\n              -- delete space token entirely, otherwise keep one space\r\n              local s = checkpair(prev, i + 1)\r\n              if s == \"\" then\r\n                settoken()  -- remove entirely\r\n              else\r\n                settoken(\"TK_SPACE\", \" \")\r\n              end\r\n            end\r\n          end\r\n          ------------------------------------------------------------\r\n        end\r\n      end\r\n    ----------------------------------------------------------------\r\n    else\r\n      error(\"unidentified token encountered\")\r\n    end\r\n    ----------------------------------------------------------------\r\n    i = i + 1\r\n  end--while\r\n  repack_tokens()\r\n  --------------------------------------------------------------------\r\n  -- processing loop (PASS 2)\r\n  --------------------------------------------------------------------\r\n  if opt_eols then\r\n    i = 1\r\n    -- aggressive EOL removal only works with most non-grammar tokens\r\n    -- optimized away because it is a rather simple scheme -- basically\r\n    -- it just checks 'real' token pairs around EOLs\r\n    if stoks[1] == \"TK_COMMENT\" then\r\n      -- first comment still existing must be shbang, skip whole line\r\n      i = 3\r\n    end\r\n    while true do\r\n      tok, info = stoks[i], sinfos[i]\r\n      --------------------------------------------------------------\r\n      if tok == \"TK_EOS\" then           -- end of stream/pass\r\n        break\r\n      --------------------------------------------------------------\r\n      elseif tok == \"TK_EOL\" then       -- consider each TK_EOL\r\n        local t1, t2 = stoks[i - 1], stoks[i + 1]\r\n        if is_realtoken[t1] and is_realtoken[t2] then  -- sanity check\r\n          local s = checkpair(i - 1, i + 1)\r\n          if s == \"\" then\r\n            settoken()  -- remove entirely\r\n          end\r\n        end\r\n      end--if tok\r\n      --------------------------------------------------------------\r\n      i = i + 1\r\n    end--while\r\n    repack_tokens()\r\n  end\r\n  --------------------------------------------------------------------\r\n  if opt_details and opt_details > 0 then print() end -- spacing\r\n  return stoks, sinfos, stoklns\r\nend",
    "type": "function"
  }, {
    "id": 10,
    "text": "local function die(msg)\r\n  print_err(\"minify: \"..msg); os.exit(1);\r\nend",
    "type": "function"
  }, {
    "id": 11,
    "text": "print_err(\"minify: \"..msg)",
    "type": "statement:functioncall"
  }, {
    "id": 12,
    "text": "os.exit(1)",
    "type": "statement:functioncall"
  }, {
    "id": 13,
    "text": "",
    "type": "blank lines"
  }, {
    "id": 14,
    "text": "local function load_file(fname)\r\n  local INF = io.open(fname, \"rb\")\r\n  if not INF then die(\"cannot open \\\"\"..fname..\"\\\" for reading\") end\r\n  local dat = INF:read(\"*a\")\r\n  if not dat then die(\"cannot read from \\\"\"..fname..\"\\\"\") end\r\n  INF:close()\r\n  return dat\r\nend",
    "type": "function"
  }, {
    "id": 15,
    "text": "local INF = io.open(fname, \"rb\")",
    "type": "statement:localassign"
  }, {
    "id": 16,
    "text": "if not INF then die(\"cannot open \\\"\"..fname..\"\\\" for reading\") end",
    "type": "statement:if"
  }, {
    "id": 17,
    "text": "die(\"cannot open \\\"\"..fname..\"\\\" for reading\")",
    "type": "statement:functioncall"
  }, {
    "id": 18,
    "text": "local dat = INF:read(\"*a\")",
    "type": "statement:localassign"
  }, {
    "id": 19,
    "text": "if not dat then die(\"cannot read from \\\"\"..fname..\"\\\"\") end",
    "type": "statement:if"
  }, {
    "id": 20,
    "text": "die(\"cannot read from \\\"\"..fname..\"\\\"\")",
    "type": "statement:functioncall"
  }, {
    "id": 21,
    "text": "INF:close()",
    "type": "statement:functioncall"
  }, {
    "id": 22,
    "text": "return",
    "type": "statement:keyword"
  }, {
    "id": 23,
    "text": "",
    "type": "blank lines"
  }, {
    "id": 24,
    "text": "local function save_file(fname, dat)\r\n  local OUTF = io.open(fname, \"wb\")\r\n  if not OUTF then die(\"cannot open \\\"\"..fname..\"\\\" for writing\") end\r\n  local status = OUTF:write(dat)\r\n  if not status then die(\"cannot write to \\\"\"..fname..\"\\\"\") end\r\n  OUTF:close()\r\nend",
    "type": "function"
  }, {
    "id": 25,
    "text": "local OUTF = io.open(fname, \"wb\")",
    "type": "statement:localassign"
  }, {
    "id": 26,
    "text": "if not OUTF then die(\"cannot open \\\"\"..fname..\"\\\" for writing\") end",
    "type": "statement:if"
  }, {
    "id": 27,
    "text": "die(\"cannot open \\\"\"..fname..\"\\\" for writing\")",
    "type": "statement:functioncall"
  }, {
    "id": 28,
    "text": "local status = OUTF:write(dat)",
    "type": "statement:localassign"
  }, {
    "id": 29,
    "text": "if not status then die(\"cannot write to \\\"\"..fname..\"\\\"\") end",
    "type": "statement:if"
  }, {
    "id": 30,
    "text": "die(\"cannot write to \\\"\"..fname..\"\\\"\")",
    "type": "statement:functioncall"
  }, {
    "id": 31,
    "text": "OUTF:close()",
    "type": "statement:functioncall"
  }, {
    "id": 32,
    "text": "",
    "type": "blank lines"
  }, {
    "id": 33,
    "text": "function minify_string(dat)\r\n\tllex.init(dat)\r\n\tllex.llex()\r\n\tlocal toklist, seminfolist, toklnlist\r\n\t= llex.tok, llex.seminfo, llex.tokln\r\n\tif option[\"opt-locals\"] then\r\n\t\toptparser.print = print  -- hack\r\n\t\tlparser.init(toklist, seminfolist, toklnlist)\r\n\t\tlocal globalinfo, localinfo = lparser.parser()\r\n\t\toptparser.optimize(option, toklist, seminfolist, globalinfo, localinfo)\r\n\tend\r\n\toptlex.print = print  -- hack\r\n\ttoklist, seminfolist, toklnlist\r\n\t\t= optlex.optimize(option, toklist, seminfolist, toklnlist)\r\n\tlocal dat = table.concat(seminfolist)\r\n\t-- depending on options selected, embedded EOLs in long strings and\r\n\t-- long comments may not have been translated to \\n, tack a warning\r\n\tif string.find(dat, \"\\r\\n\", 1, 1) or\r\n\t\tstring.find(dat, \"\\n\\r\", 1, 1) then\r\n\t\toptlex.warn.mixedeol = true\r\n\tend\r\n\treturn dat;\r\nend",
    "type": "function"
  }, {
    "id": 34,
    "text": "llex.init(dat)",
    "type": "statement:functioncall"
  }, {
    "id": 35,
    "text": "llex.llex()",
    "type": "statement:functioncall"
  }, {
    "id": 36,
    "text": "local toklist, seminfolist, toklnlist\r\n\t= llex.tok, llex.seminfo, llex.tokln",
    "type": "statement:localassign"
  }, {
    "id": 37,
    "text": "if option[\"opt-locals\"] then\r\n\t\toptparser.print = print  -- hack\r\n\t\tlparser.init(toklist, seminfolist, toklnlist)\r\n\t\tlocal globalinfo, localinfo = lparser.parser()\r\n\t\toptparser.optimize(option, toklist, seminfolist, globalinfo, localinfo)\r\n\tend",
    "type": "statement:if"
  }, {
    "id": 38,
    "text": "optparser.print = print",
    "type": "statement:assign"
  }, {
    "id": 39,
    "text": "lparser.init(toklist, seminfolist, toklnlist)",
    "type": "statement:functioncall"
  }, {
    "id": 40,
    "text": "local globalinfo, localinfo = lparser.parser()",
    "type": "statement:localassign"
  }, {
    "id": 41,
    "text": "optparser.optimize(option, toklist, seminfolist, globalinfo, localinfo)",
    "type": "statement:functioncall"
  }, {
    "id": 42,
    "text": "optlex.print = print",
    "type": "statement:assign"
  }, {
    "id": 43,
    "text": "toklist, seminfolist, toklnlist\r\n\t\t= optlex.optimize(option, toklist, seminfolist, toklnlist)",
    "type": "statement:assign"
  }, {
    "id": 44,
    "text": "local dat = table.concat(seminfolist)",
    "type": "statement:localassign"
  }, {
    "id": 45,
    "text": "if string.find(dat, \"\\r\\n\", 1, 1) or\r\n\t\tstring.find(dat, \"\\n\\r\", 1, 1) then\r\n\t\toptlex.warn.mixedeol = true\r\n\tend",
    "type": "statement:if"
  }, {
    "id": 46,
    "text": "optlex.warn.mixedeol = true",
    "type": "statement:assign"
  }, {
    "id": 47,
    "text": "",
    "type": "blank lines"
  }, {
    "id": 48,
    "text": "function minify_file(srcfl, destfl)\r\n\tlocal z = load_file(srcfl);\r\n\tz = minify_string(z);\r\n\tsave_file(destfl, z);\r\nend",
    "type": "function"
  }, {
    "id": 49,
    "text": "local z = load_file(srcfl)",
    "type": "statement:localassign"
  }, {
    "id": 50,
    "text": "z = minify_string(z)",
    "type": "statement:assign"
  }, {
    "id": 51,
    "text": "save_file(destfl, z)",
    "type": "statement:functioncall"
  }, {
    "id": 52,
    "text": "",
    "type": "blank lines"
  }, {
    "id": 53,
    "text": "function init(_z, _sourceid)\r\n  z = _z                        -- source\r\n  sourceid = _sourceid          -- name of source\r\n  I = 1                         -- lexer's position in source\r\n  ln = 1                        -- line number\r\n  tok = {}                      -- lexed token list*\r\n  seminfo = {}                  -- lexed semantic information list*\r\n  tokln = {}                    -- line numbers for messages*\r\n                                -- (*) externally visible thru' module\r\n  --------------------------------------------------------------------\r\n  -- initial processing (shbang handling)\r\n  --------------------------------------------------------------------\r\n  local p, _, q, r = find(z, \"^(#[^\\r\\n]*)(\\r?\\n?)\")\r\n  if p then                             -- skip first line\r\n    I = I + #q\r\n    addtoken(\"TK_COMMENT\", q)\r\n    if #r > 0 then inclinenumber(I, true) end\r\n  end\r\nend",
    "type": "function"
  }, {
    "id": 54,
    "text": "function llex()\r\n  local find = find\r\n  local match = match\r\n  while true do--outer\r\n    local i = I\r\n    -- inner loop allows break to be used to nicely section tests\r\n    while true do--inner\r\n      ----------------------------------------------------------------\r\n      local p, _, r = find(z, \"^([_%a][_%w]*)\", i)\r\n      if p then\r\n        I = i + #r\r\n        if kw[r] then\r\n          addtoken(\"TK_KEYWORD\", r)             -- reserved word (keyword)\r\n        else\r\n          addtoken(\"TK_NAME\", r)                -- identifier\r\n        end\r\n        break -- (continue)\r\n      end\r\n      ----------------------------------------------------------------\r\n      local p, _, r = find(z, \"^(%.?)%d\", i)\r\n      if p then                                 -- numeral\r\n        if r == \".\" then i = i + 1 end\r\n        local _, q, r = find(z, \"^%d*[%.%d]*([eE]?)\", i)\r\n        i = q + 1\r\n        if #r == 1 then                         -- optional exponent\r\n          if match(z, \"^[%+%-]\", i) then        -- optional sign\r\n            i = i + 1\r\n          end\r\n        end\r\n        local _, q = find(z, \"^[_%w]*\", i)\r\n        I = q + 1\r\n        local v = sub(z, p, q)                  -- string equivalent\r\n        if not base.tonumber(v) then            -- handles hex test also\r\n          errorline(\"malformed number\")\r\n        end\r\n        addtoken(\"TK_NUMBER\", v)\r\n        break -- (continue)\r\n      end\r\n      ----------------------------------------------------------------\r\n      local p, q, r, t = find(z, \"^((%s)[ \\t\\v\\f]*)\", i)\r\n      if p then\r\n        if t == \"\\n\" or t == \"\\r\" then          -- newline\r\n          inclinenumber(i, true)\r\n        else\r\n          I = q + 1                             -- whitespace\r\n          addtoken(\"TK_SPACE\", r)\r\n        end\r\n        break -- (continue)\r\n      end\r\n      ----------------------------------------------------------------\r\n      local r = match(z, \"^%p\", i)\r\n      if r then\r\n        buff = i\r\n        local p = find(\"-[\\\"\\'.=<>~\", r, 1, true)\r\n        if p then\r\n          -- two-level if block for punctuation/symbols\r\n          --------------------------------------------------------\r\n          if p <= 2 then\r\n            if p == 1 then                      -- minus\r\n              local c = match(z, \"^%-%-(%[?)\", i)\r\n              if c then\r\n                i = i + 2\r\n                local sep = -1\r\n                if c == \"[\" then\r\n                  sep = skip_sep(i)\r\n                end\r\n                if sep >= 0 then                -- long comment\r\n                  addtoken(\"TK_LCOMMENT\", read_long_string(false, sep))\r\n                else                            -- short comment\r\n                  I = find(z, \"[\\n\\r]\", i) or (#z + 1)\r\n                  addtoken(\"TK_COMMENT\", sub(z, buff, I - 1))\r\n                end\r\n                break -- (continue)\r\n              end\r\n              -- (fall through for \"-\")\r\n            else                                -- [ or long string\r\n              local sep = skip_sep(i)\r\n              if sep >= 0 then\r\n                addtoken(\"TK_LSTRING\", read_long_string(true, sep))\r\n              elseif sep == -1 then\r\n                addtoken(\"TK_OP\", \"[\")\r\n              else\r\n                errorline(\"invalid long string delimiter\")\r\n              end\r\n              break -- (continue)\r\n            end\r\n          --------------------------------------------------------\r\n          elseif p <= 5 then\r\n            if p < 5 then                       -- strings\r\n              I = i + 1\r\n              addtoken(\"TK_STRING\", read_string(r))\r\n              break -- (continue)\r\n            end\r\n            r = match(z, \"^%.%.?%.?\", i)        -- .|..|... dots\r\n            -- (fall through)\r\n          --------------------------------------------------------\r\n          else                                  -- relational\r\n            r = match(z, \"^%p=?\", i)\r\n            -- (fall through)\r\n          end\r\n        end\r\n        I = i + #r\r\n        addtoken(\"TK_OP\", r)  -- for other symbols, fall through\r\n        break -- (continue)\r\n      end\r\n      ----------------------------------------------------------------\r\n      local r = sub(z, i, i)\r\n      if r ~= \"\" then\r\n        I = i + 1\r\n        addtoken(\"TK_OP\", r)                    -- other single-char tokens\r\n        break\r\n      end\r\n      addtoken(\"TK_EOS\", \"\")                    -- end of stream,\r\n      return                                    -- exit here\r\n      ----------------------------------------------------------------\r\n    end--while inner\r\n  end--while outer\r\nend",
    "type": "function"
  }, {
    "id": 55,
    "text": "concat",
    "type": "global function"
  }, {
    "id": 56,
    "text": "find",
    "type": "global function"
  }, {
    "id": 57,
    "text": "exit",
    "type": "global function"
  }, {
    "id": 58,
    "text": "print_err",
    "type": "global function"
  }, {
    "id": 59,
    "text": "open",
    "type": "global function"
  }, {
    "id": 60,
    "text": "",
    "type": "variable container"
  }, {
    "id": 61,
    "text": "",
    "type": "require container"
  }, {
    "id": 62,
    "text": "",
    "type": "local variable"
  }, {
    "id": 63,
    "text": "",
    "type": "tableconstructor"
  }, {
    "id": 64,
    "text": "",
    "type": "table assign node"
  }, {
    "id": 65,
    "text": "",
    "type": "tableconstructor"
  }, {
    "id": 66,
    "text": "",
    "type": "table assign node"
  }, {
    "id": 67,
    "text": "",
    "type": "tableconstructor"
  }, {
    "id": 68,
    "text": "",
    "type": "table assign node"
  }, {
    "id": 69,
    "text": "",
    "type": "tableconstructor"
  }, {
    "id": 70,
    "text": "",
    "type": "table assign node"
  }, {
    "id": 71,
    "text": "",
    "type": "tableconstructor"
  }, {
    "id": 72,
    "text": "",
    "type": "table assign node"
  }, {
    "id": 73,
    "text": "",
    "type": "tableconstructor"
  }, {
    "id": 74,
    "text": "",
    "type": "local variable"
  }, {
    "id": 75,
    "text": "",
    "type": "tableconstructor"
  }, {
    "id": 76,
    "text": "",
    "type": "table assign node"
  }, {
    "id": 77,
    "text": "",
    "type": "_prefixexp"
  }, {
    "id": 78,
    "text": "",
    "type": "table assign node"
  }, {
    "id": 79,
    "text": "",
    "type": "_prefixexp"
  }, {
    "id": 80,
    "text": "",
    "type": "table assign node"
  }, {
    "id": 81,
    "text": "",
    "type": "_prefixexp"
  }, {
    "id": 82,
    "text": "",
    "type": "table assign node"
  }, {
    "id": 83,
    "text": "",
    "type": "_prefixexp"
  }, {
    "id": 84,
    "text": "",
    "type": "table assign node"
  }, {
    "id": 85,
    "text": "",
    "type": "_prefixexp"
  }, {
    "id": 86,
    "text": "",
    "type": "table assign node"
  }, {
    "id": 87,
    "text": "",
    "type": "_prefixexp"
  }, {
    "id": 88,
    "text": "",
    "type": "table assign node"
  }, {
    "id": 89,
    "text": "",
    "type": "_prefixexp"
  }, {
    "id": 90,
    "text": "",
    "type": "table assign node"
  }, {
    "id": 91,
    "text": "",
    "type": "_prefixexp"
  }, {
    "id": 92,
    "text": "",
    "type": "require local variable"
  }, {
    "id": 93,
    "text": "",
    "type": "require local variable"
  }, {
    "id": 94,
    "text": "",
    "type": "require local variable"
  }, {
    "id": 95,
    "text": "",
    "type": "require local variable"
  } ]
}